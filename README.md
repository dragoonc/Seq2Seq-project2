# Seq2Seq demo by pytorch
This demo contains tutorials covering implementing sequence-to-sequence (seq2seq) model suing Pytorch 1.1 , Tensorflow 1.14 and TorchText 0.4 on Python 3.6.

-Brief
A sequence-to-sequence model implemented by pytorch and torchtext. This model is designed to translate English into Chinese.

An update of attention-based model is modified to the original Seq2Seq model.

-Getting Started
We will pass the installation of PyTorch and TorchText as there are plenty of instructions on the PyTorch website and others.

-------------------------------------------------------------------------------------

## Dataset
--------

Whole datasets contains 3 parts including a training one named as train1.txt, a testing one named as test1.txt and a valid one named as valid1.txt.

Textual data in each one of them is formed as:
An English word .    一个 汉语 词汇 。

Chinese sentences in original datasets were continuous, however we have to separate every word and punctuation in these sentences with spaces as preprocessing. The slice tool we used here is with stanfordcorenlp, links and references could be found after.

Original form: ['An English word.'] ['一个汉语词汇。']

After sliced: ['An','English','word','.'] ['一个','汉语','词汇','。']

### Preparation

The source language contains English sentences that are continuous with spaces between words and a full stop at the end, and Chinses sentences that are continuous with only a full stop at the end. We need to seperate all elements by spaces (all punctuations included), here we would like to use stanfordcorenlp as slice tool to preprocess the raw data.

The source code of Stanford CoreNLP(SCN) is realized in Java, which provides the server mode for interaction. stanfordcorenlp is a Python toolkit that encapsulates SCN. Stanford officially released the Python version, which can be installed directly. For details, please check the [link](https://stanfordnlp.github.io/stanfordnlp/) in Reference.

After slicing, the natural language in text datasets should be seperated in 2 files which are labeled as .en and .zh respectively. File .en contains source language namely English text. File .zh contains target language that should be Chinese text. This work could be done by Seperation in project.

### Preprocess

In order to convert the text into a word sequence that the model could read, we need to map these Chinese words and English words to integer numbers between 0 and src.size(). We first determine the vocabulary according to the word frequency sequence, and then save the vocabulary to 2 independent vocab files.

After the vocabulary is determined, the training files, test files are converted to word index according to the vocabulary files. The index of each word is its line number in the vocabulary file. Hence that after running the corresponding program, the original Chinese and English files are changed into 2 files only contain with numbers.

## Batching
--------

In PTB data, there is context relation between sentences which could be directly connected into a larger paragrahp. In the training samples of translation, each sentence pair is usually trained as independent data. As the length of each sentence is different, when these sentences are put in the same batch, the shorter sentences need to be filed to the same length as the longest one in the same batch. The position used to fill the length is named as padding. In TensorFlow, padded_batch from tf.data.Dataset function should solve this problem.

When reading the data, the RNN will bring the content of the filling position into the calculation as other content, so in order not to let the filling affect the training, it may affect the calculation of the training result and loss, so the following two solutions are needed:

First, the RNN should skip the calculation of this position when reading and filling. For example, if the encoder processes the fill input as normal input when reading the fill, the last bit hidden sequence generated after reading "b1b200" is different from the hidden state after reading "b1b2", which will produce wrong results. Generally speaking, the result generated by inputting original data + padding data changes through encoder prediction.

But tensorflow provides tf.nn.dynamic_rnn function to realize this function conveniently and solve this problem. dynamic_rnn reads two inputs for each batch's data.

1. input data content (dimension is [batch_size, time])

2. length of input data (dimension is [time])

For each piece of data in the input batch, after reading the content of the corresponding length, dynamic_rnn skips the later input and directly copies the calculation results of the previous step to the later time. This can ensure that the existence of padding does not affect the effect of the model. In other word, the length of a sentence, i.e. time, is used to control this. The maximum sequence length of each batch does not need to be the same when using dynamic_rnn. In the training process, dynamic_rnn will dynamically expand to the required number of layers according to the maximum length of each batch, in fact, there is no relationship between the maximum length of each batch itself, and the function will automatically adjust dynamically.

Second, when designing the loss function, we need to set the loss weight of the filling position to 0, so that the prediction generated in the filling position will not affect the calculation of the gradient.

The code in MakeDataset uses tf.data.Dataset.padded_batch to fill and batch, and records the sequence length of each sentence as input to dynamic_rnn.


## Seq2Seq model
--------

### Model training

As the main part of RNN, LSTM shares paramethers between Softmax layer and the word vector layer, adding futures as following:

1. An RNN is used as the encoder
2. Using Dataset to read data dynamically instead of directly reading all data into memory(feature of Dataset)
3. Each batch is completely independent, and there is no need to transfer state between batches(no transfer relationship between each sentences as it is not a complete sentence in file)
4. The model parameters are saved to a checkpoint in every 200 steps in training, for later testing.

As both Encoder and Decoder use same LSTM structure, so we define a single Model class to describe S2S model.

### Decoding or inference

Program S2S finished the training of translation model which was well trained and saved in checkpoint.

The following will show how to read the model in checkpoint and translate a new sentence, which could be referred to as decoding or inference. In training process the Decoder is able to read the whole objective sentence in input, on the contrary in decoding process there is no showing of target sentence but only input sentence in model. The detail of processing could be describ as:
1. Decoder read <sos> and predict the first word in objective sentence;
2. Input the copy of predicted word to the next step and predict the second word;
3. Until reach the <eos>.

The whole process needs a loop to realize. In TensorFlow it could be done by tf.while_loop.


## Attention mechanism
--------

The classic Seq2Seq model contains the Encoder and the Decoder, the usual way is to encode an input sentence into a fixed size state, and then take it as the initial state of Decoder (which could also be the input of every moment of course), but such a state is the same for all moments in Decoder.

The reason of Attention needed is very intuitive. The attention of human brain is different in different parts. The general model is regarded as the model that the attention of every parts are all the same one, and the attention-based model here is different for different parts.

The updating after attention model:

1. The modification is to change the multi-layer RNN structure of the original Encoder into a bidirectional RNN;
2. The forward functions in Encoder and Decoder have been slightly modified;
3. The decoding program has been modified adding with attention mechanism (the initial state of decoding here is no longer the same as last hidden state of Encoder, but with null)

## Testing
--------

The tests of original S2S model and attention-based S2S model share the same input of test, which is an English sentence you would like to translate. Except for the accuracy of translation result and training efficiency, the main difference of 2 results with same input is that the later one contains punctuation mark.

## Manual
--------

### Dataset preprocessing
-stanfordcorenlp
--------
The original textual file of dataset contains both source(English) sentences and target language(Chinese) which are formed as continuous sentences in 2 columns. It should be looked like this:

`He ran.	他跑了。`
`Hop in.	跳进来。`

Our first task aims at slicing the continuous sentences with spaces between each elements(words and punctuations included). A general used tool named [stanfordcorenlp](https://stanfordnlp.github.io/stanfordnlp/) could finish this task nice and smooth.

You may want to install and use this module by doing this:

`pip install --upgrade stanfordnlp`
`conda install --upgrade stanfordnlp`
`from stanfordcorenlp import StanfordCoreNLP`

-seperation.py
--------
The after-processed text file is as followed:

`He ran .	他 跑 了 。`
`Hop in .	跳 进 来 。`

For more conveniet follow-up processing, you may like to seperating different languages into 2 text files.

Function in speration.py seperates the original text into .en and .zh files for later vocab making.

### making vocabs

vocab.py transforms the text data to word sequences which is the input to the model, all Chinese words or characters and English words are mapped into integer numbers respectively.

### indexing

indexing.py changes words into word number according to the vocabulary that has been determined.

### batching

makedataset.py aims at filling and batching, and recording the length of sequence in each sentence as input to dynamic_rnn.

### seq2seq

-seq2seq.py
--------
Realizing a completed Seq2Seq model and save parameters of model in checkpoints on the process of training, for laterly testing.

-attention_s2s.py
--------
Adding the attention model to original seq2seq model.

### decoding

-decoding.py.
--------
The input of CHECKPOINT_PATH should point to the checkpoints that have been saved in seq2seq.py. "seq2seq_ckpt-x" means the model is saved at the checkpoint after x steps.

-attention_decoding.py
--------
Adding attention model to the original decoding processing.
Same CHECKPOINT_PATH input as decoding.py.

## Results
--------

The results of testing consist 2 parts: indices of word sequence and the corresponding sequence of target language. Our testing is simple. Different results are showed as followed(with and without attention mechanism):

input:

`This is a test .`
`[91, 13, 9, 682, 4, 2]`

output:

`[1 13 6 12 361 2]`
`这是个实验`

input:

`This is a test .`
`[91, 13, 9, 682, 4, 2]`

output:

`[1 13 6 9 12 361 5 2]`
`这是一个实验。`

We only training this model for 500 steps for just studying, so results could not be so satisfying.

## Reference
Sutskever I, Vinyals O, Le Q V. Sequence to sequence learning with neural networks[J]. Advances in NIPS, 2014.
https://github.com/keras-team/keras/blob/master/examples/lstm_seq2seq.py
https://stanfordnlp.github.io/stanfordnlp/
https://github.com/bentrevett/pytorch-seq2seq
https://pytorch-cn.readthedocs.io/zh/latest/
https://blog.csdn.net/leo_95/article/details/87708267#Field_7
